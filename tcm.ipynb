{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1CtlM0QTq_cJAjSKKjymzc4yFMxxxVaZG",
      "authorship_tag": "ABX9TyMehadzkwbtmwqkdl+0yGE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangbxj1234/huazi/blob/main/tcm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 准备"
      ],
      "metadata": {
        "id": "_JKtNQkGNff5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install compressai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dIWZTzuTubkP",
        "outputId": "ac4e2fe0-e694-4bc5-d420-4468bfdb1a00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting compressai\n",
            "  Downloading compressai-1.2.6.tar.gz (163 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from compressai) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from compressai) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from compressai) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from compressai) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from compressai) (3.7.1)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from compressai) (2.4.1+cu121)\n",
            "Collecting torch-geometric>=2.3.0 (from compressai)\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from compressai) (4.12.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from compressai) (0.19.1+cu121)\n",
            "Collecting pytorch-msssim (from compressai)\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from compressai) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->compressai) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->compressai) (3.10.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->compressai) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->compressai) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->compressai) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (10.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->compressai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->compressai) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->compressai) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->compressai) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.12.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->compressai) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->compressai) (1.3.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Building wheels for collected packages: compressai\n",
            "  Building wheel for compressai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressai: filename=compressai-1.2.6-cp310-cp310-linux_x86_64.whl size=405682 sha256=db5633ae93d5849b81c8014591198ef2939be832ccc89f7c39dbee2823c68f97\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/e1/85/87edc5d40a531877f35ba1cfc8f66e2e76d49d4845f57c0f46\n",
            "Successfully built compressai\n",
            "Installing collected packages: pytorch-msssim, torch-geometric, compressai\n",
            "Successfully installed compressai-1.2.6 pytorch-msssim-1.0.0 torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "#from compressai.ops import ste_round\n",
        "from compressai.models.utils import conv, deconv\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from compressai.layers import (\n",
        "    AttentionBlock,\n",
        "    ResidualBlock,\n",
        "    ResidualBlockUpsample,\n",
        "    ResidualBlockWithStride,\n",
        "    conv3x3,\n",
        "    subpel_conv3x3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LITmOF-pWkXI",
        "outputId": "ab355d91-ad58-4552-f7ff-47a523aaf54c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @amp.autocast(enabled=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "from timm.models.layers import trunc_normal_, DropPath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYWe0Jen9Etf",
        "outputId": "a99bb415-ac3f-4073-fb82-88d3b08a7e94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conv1x1(in_ch: int, out_ch: int, stride: int = 1) -> nn.Module:\n",
        "    \"\"\"1x1 convolution.\"\"\"\n",
        "    return nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride)\n",
        "def conv(in_channels, out_channels, kernel_size=5, stride=2):\n",
        "    return nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        stride=stride,\n",
        "        padding=kernel_size // 2,\n",
        "    )\n",
        "\n",
        "\n",
        "class WMSA(nn.Module):\n",
        "    \"\"\" Self-attention module in Swin Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, head_dim, window_size, type):\n",
        "        super(WMSA, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.head_dim = head_dim\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.n_heads = input_dim//head_dim\n",
        "        self.window_size = window_size\n",
        "        self.type=type\n",
        "        self.embedding_layer = nn.Linear(self.input_dim, 3*self.input_dim, bias=True)\n",
        "        self.relative_position_params = nn.Parameter(torch.zeros((2 * window_size - 1)*(2 * window_size -1), self.n_heads))\n",
        "\n",
        "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
        "\n",
        "        trunc_normal_(self.relative_position_params, std=.02)\n",
        "        self.relative_position_params = torch.nn.Parameter(self.relative_position_params.view(2*window_size-1, 2*window_size-1, self.n_heads).transpose(1,2).transpose(0,1))\n",
        "\n",
        "    def generate_mask(self, h, w, p, shift):\n",
        "        \"\"\" generating the mask of SW-MSA\n",
        "        Args:\n",
        "            shift: shift parameters in CyclicShift.\n",
        "        Returns:\n",
        "            attn_mask: should be (1 1 w p p),\n",
        "        \"\"\"\n",
        "        attn_mask = torch.zeros(h, w, p, p, p, p, dtype=torch.bool, device=self.relative_position_params.device)\n",
        "        if self.type == 'W':\n",
        "            return attn_mask\n",
        "\n",
        "        s = p - shift\n",
        "        attn_mask[-1, :, :s, :, s:, :] = True\n",
        "        attn_mask[-1, :, s:, :, :s, :] = True\n",
        "        attn_mask[:, -1, :, :s, :, s:] = True\n",
        "        attn_mask[:, -1, :, s:, :, :s] = True\n",
        "        attn_mask = rearrange(attn_mask, 'w1 w2 p1 p2 p3 p4 -> 1 1 (w1 w2) (p1 p2) (p3 p4)')\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward pass of Window Multi-head Self-attention module.\n",
        "        Args:\n",
        "            x: input tensor with shape of [b h w c];\n",
        "            attn_mask: attention mask, fill -inf where the value is True;\n",
        "        Returns:\n",
        "            output: tensor shape [b h w c]\n",
        "        \"\"\"\n",
        "        if self.type!='W': x = torch.roll(x, shifts=(-(self.window_size//2), -(self.window_size//2)), dims=(1,2))\n",
        "        x = rearrange(x, 'b (w1 p1) (w2 p2) c -> b w1 w2 p1 p2 c', p1=self.window_size, p2=self.window_size)\n",
        "        h_windows = x.size(1)\n",
        "        w_windows = x.size(2)\n",
        "        x = rearrange(x, 'b w1 w2 p1 p2 c -> b (w1 w2) (p1 p2) c', p1=self.window_size, p2=self.window_size)\n",
        "        qkv = self.embedding_layer(x)\n",
        "        q, k, v = rearrange(qkv, 'b nw np (threeh c) -> threeh b nw np c', c=self.head_dim).chunk(3, dim=0)\n",
        "        sim = torch.einsum('hbwpc,hbwqc->hbwpq', q, k) * self.scale\n",
        "        sim = sim + rearrange(self.relative_embedding(), 'h p q -> h 1 1 p q')\n",
        "        if self.type != 'W':\n",
        "            attn_mask = self.generate_mask(h_windows, w_windows, self.window_size, shift=self.window_size//2)\n",
        "            sim = sim.masked_fill_(attn_mask, float(\"-inf\"))\n",
        "\n",
        "        probs = nn.functional.softmax(sim, dim=-1)\n",
        "        output = torch.einsum('hbwij,hbwjc->hbwic', probs, v)\n",
        "        output = rearrange(output, 'h b w p c -> b w p (h c)')\n",
        "        output = self.linear(output)\n",
        "        output = rearrange(output, 'b (w1 w2) (p1 p2) c -> b (w1 p1) (w2 p2) c', w1=h_windows, p1=self.window_size)\n",
        "\n",
        "        if self.type!='W': output = torch.roll(output, shifts=(self.window_size//2, self.window_size//2), dims=(1,2))\n",
        "        return output\n",
        "\n",
        "    def relative_embedding(self):\n",
        "        cord = torch.tensor(np.array([[i, j] for i in range(self.window_size) for j in range(self.window_size)]))\n",
        "        relation = cord[:, None, :] - cord[None, :, :] + self.window_size -1\n",
        "        return self.relative_position_params[:, relation[:,:,0].long(), relation[:,:,1].long()]\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, head_dim, window_size, drop_path, type='W', input_resolution=None):\n",
        "        \"\"\" SwinTransformer Block\n",
        "        \"\"\"\n",
        "        super(Block, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        assert type in ['W', 'SW']\n",
        "        self.type = type\n",
        "        self.ln1 = nn.LayerNorm(input_dim)\n",
        "        self.msa = WMSA(input_dim, input_dim, head_dim, window_size, self.type)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.ln2 = nn.LayerNorm(input_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4 * input_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * input_dim, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.msa(self.ln1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class ConvTransBlock(nn.Module):\n",
        "    def __init__(self, conv_dim, trans_dim, head_dim, window_size, drop_path, type='W'):\n",
        "        \"\"\" SwinTransformer and Conv Block\n",
        "        \"\"\"\n",
        "        super(ConvTransBlock, self).__init__()\n",
        "        self.conv_dim = conv_dim\n",
        "        self.trans_dim = trans_dim\n",
        "        self.head_dim = head_dim\n",
        "        self.window_size = window_size\n",
        "        self.drop_path = drop_path\n",
        "        self.type = type\n",
        "        assert self.type in ['W', 'SW']\n",
        "        self.trans_block = Block(self.trans_dim, self.trans_dim, self.head_dim, self.window_size, self.drop_path, self.type)\n",
        "        self.conv1_1 = nn.Conv2d(self.conv_dim+self.trans_dim, self.conv_dim+self.trans_dim, 1, 1, 0, bias=True)\n",
        "        self.conv1_2 = nn.Conv2d(self.conv_dim+self.trans_dim, self.conv_dim+self.trans_dim, 1, 1, 0, bias=True)\n",
        "\n",
        "        self.conv_block = ResidualBlock(self.conv_dim, self.conv_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_x, trans_x = torch.split(self.conv1_1(x), (self.conv_dim, self.trans_dim), dim=1)\n",
        "        conv_x = self.conv_block(conv_x) + conv_x\n",
        "        trans_x = Rearrange('b c h w -> b h w c')(trans_x)\n",
        "        trans_x = self.trans_block(trans_x)\n",
        "        trans_x = Rearrange('b h w c -> b c h w')(trans_x)\n",
        "        res = self.conv1_2(torch.cat((conv_x, trans_x), dim=1))\n",
        "        x = x + res\n",
        "        return x"
      ],
      "metadata": {
        "id": "4v6dKdLZW-v4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新的对称法"
      ],
      "metadata": {
        "id": "FdTfp72tGznv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "import numpy as np\n",
        "import math\n",
        "from compressai.models import CompressionModel\n",
        "\n",
        "# 定义完整图像压缩网络\n",
        "#class symmetry_NET(nn.Module):\n",
        "class symmetry_NET(CompressionModel):\n",
        "    def __init__(self, config, N, M,drop_path_rate=0):\n",
        "        super(symmetry_NET, self).__init__()\n",
        "        self.head_dim = [8, 16, 32, 32, 16, 8]\n",
        "        self.window_size = 8\n",
        "        dim = N\n",
        "        self.M = M\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(config))]\n",
        "        begin = 0\n",
        "\n",
        "        self.m_down1 = [ConvTransBlock(dim, dim, self.head_dim[0], self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW')\n",
        "                      for i in range(config[0])] + \\\n",
        "                      [ResidualBlockWithStride(2*N, 2*N, stride=2)]\n",
        "        self.m_down2 = [ConvTransBlock(dim, dim, self.head_dim[1], self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW')\n",
        "                      for i in range(config[1])] + \\\n",
        "                      [ResidualBlockWithStride(2*N, 2*N, stride=2)]\n",
        "        self.m_down3 = [ConvTransBlock(dim, dim, self.head_dim[2], self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW')\n",
        "                      for i in range(config[2])] + \\\n",
        "                      [conv3x3(2*N, M, stride=2)]\n",
        "\n",
        "        self.m_up1 = [ConvTransBlock(dim, dim, self.head_dim[3], self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW')\n",
        "                      for i in range(config[3])] + \\\n",
        "                      [ResidualBlockUpsample(2*N, 2*N, 2)]\n",
        "        self.m_up2 = [ConvTransBlock(dim, dim, self.head_dim[4], self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW')\n",
        "                      for i in range(config[4])] + \\\n",
        "                      [ResidualBlockUpsample(2*N, 2*N, 2)]\n",
        "        self.m_up3 = [ConvTransBlock(dim, dim, self.head_dim[5], self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW')\n",
        "                      for i in range(config[5])] + \\\n",
        "                      [subpel_conv3x3(2*N, 3, 2)]\n",
        "\n",
        "        self.g_a = nn.Sequential(*[ResidualBlockWithStride(3, 2*N, 2)] + self.m_down1 + self.m_down2 + self.m_down3)\n",
        "\n",
        "\n",
        "        self.g_s = nn.Sequential(*[ResidualBlockUpsample(M, 2*N, 2)] + self.m_up1 + self.m_up2 + self.m_up3)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.g_a(x)\n",
        "        x_hat = self.g_s(y)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "N = 128  # Number of channels for the network\n",
        "M = 320  # Output channels\n",
        "num_epochs = 10  # Number of training epochs\n",
        "learning_rate = 0.0001\n",
        "batch_size = 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "#cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "#loader = DataLoader(cifar10, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "kodak= '/content/drive/MyDrive/huawei/kodak'\n",
        "loader = DataLoader(kodak, batch_size=2, shuffle=True)\n",
        "\n",
        "model = symmetry_NET(config=[2,2,2,2,2,2], N=N, M=M)\n",
        "model.train()  # Set to training mode\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for images, _ in loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, images)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "lLi12vsBG2Hs",
        "outputId": "28b82360-3bed-4a97-e5ed-2887e0d142e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "conv2d() received an invalid combination of arguments - got (str, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-246e16d42b94>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-246e16d42b94>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/compressai/layers/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 454\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    455\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (str, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()  # Set to evaluation mode\n",
        "data_iter = iter(loader)\n",
        "image, _ = next(data_iter)\n",
        "\n",
        "# Forward pass for reconstruction\n",
        "with torch.no_grad():\n",
        "    reconstructed_image = model(image)\n",
        "\n",
        "# Visualize original and reconstructed images\n",
        "plt.subplot(1, 2, 1)\n",
        "# 修改此行\n",
        "plt.imshow(image[0].permute(1, 2, 0).numpy())\n",
        "plt.title('Original Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(reconstructed_image[0].squeeze().permute(1, 2, 0).numpy())\n",
        "plt.title('Reconstructed Image')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3KV1fbzNH0w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 选一部分插值然后直通decoder"
      ],
      "metadata": {
        "id": "l5fqE0WYNwd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 定义完整图像压缩网络\n",
        "class ImageCompressionNetwork1(nn.Module):\n",
        "    def __init__(self, N, M):\n",
        "        super(ImageCompressionNetwork1, self).__init__()\n",
        "        act=nn.ReLU\n",
        "        self.attention=AttentionBlock(M)\n",
        "        self.g_a = nn.Sequential(\n",
        "            conv(3, N),  # Module 1\n",
        "            ResidualBottleneckBlock(N),  # Module 2\n",
        "            ResidualBottleneckBlock(N),  # Module 3\n",
        "            ResidualBottleneckBlock(N),  # Module 4\n",
        "            conv(N, N),  # Module 5\n",
        "            ResidualBottleneckBlock(N),  # Module 6\n",
        "            ResidualBottleneckBlock(N),  # Module 7\n",
        "            ResidualBottleneckBlock(N),  # Module 8\n",
        "            AttentionBlock(N),  # Module 9\n",
        "            conv(N, N),  # Module 10\n",
        "            ResidualBottleneckBlock(N),  # Module 11\n",
        "            ResidualBottleneckBlock(N),  # Module 12\n",
        "            ResidualBottleneckBlock(N),  # Module 13\n",
        "            conv(N, M),  # Module 14\n",
        "            AttentionBlock(M),  # Module 15\n",
        "        )\n",
        "        self.g_s = nn.Sequential(\n",
        "            AttentionBlock(M),\n",
        "            deconv(M, N),\n",
        "            # injector 들어갈 구간\n",
        "            ResidualBottleneck(N, act=act),\n",
        "            ResidualBottleneck(N, act=act),\n",
        "            ResidualBottleneck(N, act=act),\n",
        "            deconv(N, N),\n",
        "            AttentionBlock(N),\n",
        "            # extractor 들어갈 구간\n",
        "            # injector 들어갈 구간\n",
        "            ResidualBottleneck(N, act=act),\n",
        "            ResidualBottleneck(N, act=act),\n",
        "            ResidualBottleneck(N, act=act),\n",
        "            deconv(N, N),\n",
        "            ResidualBottleneck(N, act=act),\n",
        "            ResidualBottleneck(N, act=act),\n",
        "            ResidualBottleneck(N, act=act),\n",
        "            deconv(N, 13)\n",
        "        )\n",
        "        self.g_f= nn.Sequential(\n",
        "            AttentionBlock(16),\n",
        "            nn.Conv2d(16, 3, kernel_size=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "    # 计算左上1/4区域的尺寸\n",
        "        h, w = x.size(2), x.size(3)\n",
        "        quarter_h, quarter_w = h // 4, w // 4\n",
        "\n",
        "    # 提取左上1/4区域\n",
        "        left_top_quarter = x[:, :, :quarter_h, :quarter_w]\n",
        "\n",
        "    # 转换左上1/4区域为灰度图像\n",
        "        gray_quarter = left_top_quarter.mean(dim=1, keepdim=True)  # 生成单通道\n",
        "\n",
        "    # 复制到3通道，形成灰度图像\n",
        "        gray_quarter = gray_quarter.repeat(1, 3, 1, 1)  # 变为3通道\n",
        "\n",
        "    # 创建一个与x相同大小的零矩阵\n",
        "        black_white_image = torch.zeros_like(x)\n",
        "\n",
        "    # 将灰度区域放回到相应的位置\n",
        "        black_white_image[:, :, :quarter_h, :quarter_w] = gray_quarter\n",
        "\n",
        "    # 融合 x_hat 和黑白图像\n",
        "        y = self.g_a(x)\n",
        "        x_hat = self.g_s(y)\n",
        "        fused_output = torch.cat((x_hat,black_white_image),dim=1)  # 或者根据需要选择融合方式\n",
        "\n",
        "    # 继续经过 g_a 进行特征提取\n",
        "        final_output = self.g_f(fused_output)\n",
        "        return final_output\n",
        "\n",
        "\n",
        "\n",
        "# Assuming your ImageCompressionNetwork1 is already defined as above\n",
        "\n",
        "# Load CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "loader = torch.utils.data.DataLoader(cifar10, batch_size=1, shuffle=True)\n",
        "\n",
        "# Instantiate model\n",
        "model = ImageCompressionNetwork1(N=192, M=320)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Assuming your ImageCompressionNetwork1 class is defined here\n",
        "\n",
        "# Hyperparameters\n",
        "N = 192  # Number of channels for the network\n",
        "M = 320  # Output channels\n",
        "num_epochs = 10  # Number of training epochs\n",
        "learning_rate = 0.0001\n",
        "batch_size = 64\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "loader = DataLoader(cifar10, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Instantiate model\n",
        "model = ImageCompressionNetwork1(N=N, M=M)\n",
        "model.train()  # Set to training mode\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for images, _ in loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, images)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehRsHixBOAtr",
        "outputId": "41f69c3f-c6e8-4041-f3f9-ee5599974c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 41416359.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Loss: 0.0069\n",
            "Epoch [2/10], Loss: 0.0038\n",
            "Epoch [3/10], Loss: 0.0021\n",
            "Epoch [4/10], Loss: 0.0024\n",
            "Epoch [5/10], Loss: 0.0022\n",
            "Epoch [6/10], Loss: 0.0021\n",
            "Epoch [7/10], Loss: 0.0014\n",
            "Epoch [8/10], Loss: 0.0011\n",
            "Epoch [9/10], Loss: 0.0011\n",
            "Epoch [10/10], Loss: 0.0011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()  # Set to evaluation mode\n",
        "data_iter = iter(loader)\n",
        "image, _ = next(data_iter)\n",
        "\n",
        "# Forward pass for reconstruction\n",
        "with torch.no_grad():\n",
        "    reconstructed_image = model(image)\n",
        "\n",
        "# Visualize original and reconstructed images\n",
        "plt.subplot(1, 2, 1)\n",
        "# 修改此行\n",
        "plt.imshow(image[0].permute(1, 2, 0).numpy())\n",
        "plt.title('Original Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(reconstructed_image[0].squeeze().permute(1, 2, 0).numpy())\n",
        "plt.title('Reconstructed Image')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5paROWGmOHoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ed844b-2368-4041-e9ed-3229ffe115ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}